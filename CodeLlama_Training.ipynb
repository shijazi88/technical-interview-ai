{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\udd25 Technical Interview AI - CodeLlama Edition\n",
        "\n",
        "**Upgraded Model: CodeLlama-7B-Instruct**\n",
        "- \ud83e\udde0 60x more parameters than DialoGPT\n",
        "- \ud83d\udcbb Built specifically for coding\n",
        "- \ud83c\udfaf Perfect for technical interviews\n",
        "- \u26a1 15-20 minutes training on A100\n",
        "\n",
        "**Your GitHub Repository:** Update with your repo URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \ud83d\udd04 Get latest code from GitHub\n",
        "REPO_URL = 'https://github.com/YOUR_USERNAME/technical-interview-ai'\n",
        "PROJECT_DIR = 'interview-ai'\n",
        "\n",
        "import os\n",
        "if os.path.exists(PROJECT_DIR):\n",
        "    print(\"\ud83d\udd04 Updating to latest code...\")\n",
        "    %cd $PROJECT_DIR\n",
        "    !git pull origin main\n",
        "else:\n",
        "    print(\"\ud83d\udce5 Cloning repository...\")\n",
        "    !git clone $REPO_URL $PROJECT_DIR\n",
        "    %cd $PROJECT_DIR\n",
        "\n",
        "!ls -la *.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \ud83d\udce6 Install packages for CodeLlama\n",
        "!pip install -q transformers>=4.35.0 peft>=0.6.0 accelerate bitsandbytes datasets torch\n",
        "\n",
        "# Check GPU\n",
        "import torch\n",
        "print(f\"\ud83d\udd25 GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"\ud83d\udcbe VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "if torch.cuda.get_device_properties(0).total_memory < 20e9:\n",
        "    print(\"\u26a0\ufe0f Warning: Less than 20GB VRAM - CodeLlama might not fit\")\n",
        "    print(\"\ud83d\udca1 Consider using Mistral-7B-Instruct instead\")\n",
        "else:\n",
        "    print(\"\u2705 Perfect! Enough VRAM for CodeLlama-7B\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \ud83d\udd25 TRAIN WITH CODELLAMA (15-20 minutes)\n",
        "!python colab_training_pipeline.py \\\n",
        "    --model_name \"codellama/CodeLlama-7b-Instruct-hf\" \\\n",
        "    --num_scenarios 100 \\\n",
        "    --epochs 3 \\\n",
        "    --batch_size 2 \\\n",
        "    --learning_rate 2e-4 \\\n",
        "    --warmup_steps 100 \\\n",
        "    --max_length 2048\n",
        "\n",
        "print(\"\ud83c\udf89 CodeLlama training completed!\")\n",
        "print(\"\ud83c\udfc6 You now have a professional-grade technical interview AI!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \ud83e\uddea Test your upgraded AI\n",
        "from technical_interview_bot import TechnicalInterviewBot\n",
        "\n",
        "# Load your trained CodeLlama model\n",
        "bot = TechnicalInterviewBot('./technical_interview_model')\n",
        "\n",
        "# Test with a Python interview\n",
        "response = bot.start_interview(\n",
        "    programming_language='python',\n",
        "    experience_level='senior',\n",
        "    candidate_name='CodeLlama Test'\n",
        ")\n",
        "\n",
        "print(\"\ud83e\udd16 AI Interviewer:\")\n",
        "print(response)\n",
        "\n",
        "print(\"\\n\ud83c\udf89 Your CodeLlama AI is ready!\")\n",
        "print(\"\ud83d\udca1 Much smarter than DialoGPT for technical questions!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}