{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ”¥ Technical Interview AI - CodeLlama Edition\n",
        "\n",
        "**Upgraded Model: CodeLlama-7B-Instruct**\n",
        "- ðŸ§  60x more parameters than DialoGPT\n",
        "- ðŸ’» Built specifically for coding\n",
        "- ðŸŽ¯ Perfect for technical interviews\n",
        "- âš¡ 15-20 minutes training on A100\n",
        "\n",
        "**Your GitHub Repository:** Update with your repo URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ðŸ”„ Get latest code from GitHub\n",
        "REPO_URL = 'https://github.com/shijazi88/technical-interview-ai'\n",
        "PROJECT_DIR = 'interview-ai'\n",
        "\n",
        "import os\n",
        "if os.path.exists(PROJECT_DIR):\n",
        "    print(\"ðŸ”„ Updating to latest code...\")\n",
        "    %cd $PROJECT_DIR\n",
        "    !git pull origin main\n",
        "else:\n",
        "    print(\"ðŸ“¥ Cloning repository...\")\n",
        "    !git clone $REPO_URL $PROJECT_DIR\n",
        "    %cd $PROJECT_DIR\n",
        "\n",
        "!ls -la *.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ðŸ“¦ Install packages for CodeLlama\n",
        "!pip install -q transformers>=4.35.0 peft>=0.6.0 accelerate bitsandbytes datasets torch\n",
        "\n",
        "# Check GPU\n",
        "import torch\n",
        "print(f\"ðŸ”¥ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"ðŸ’¾ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "if torch.cuda.get_device_properties(0).total_memory < 20e9:\n",
        "    print(\"âš ï¸ Warning: Less than 20GB VRAM - CodeLlama might not fit\")\n",
        "    print(\"ðŸ’¡ Consider using Mistral-7B-Instruct instead\")\n",
        "else:\n",
        "    print(\"âœ… Perfect! Enough VRAM for CodeLlama-7B\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ðŸ”¥ TRAIN WITH CODELLAMA (15-20 minutes)\n",
        "!python colab_training_pipeline.py \\\n",
        "    --model_name \"codellama/CodeLlama-7b-Instruct-hf\" \\\n",
        "    --num_scenarios 100 \\\n",
        "    --epochs 3 \\\n",
        "    --batch_size 2 \\\n",
        "    --learning_rate 2e-4 \\\n",
        "    --warmup_steps 100 \\\n",
        "    --max_length 2048\n",
        "\n",
        "print(\"ðŸŽ‰ CodeLlama training completed!\")\n",
        "print(\"ðŸ† You now have a professional-grade technical interview AI!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ðŸ§ª Test your upgraded AI\n",
        "from technical_interview_bot import TechnicalInterviewBot\n",
        "\n",
        "# Load your trained CodeLlama model\n",
        "bot = TechnicalInterviewBot('./technical_interview_model')\n",
        "\n",
        "# Test with a Python interview\n",
        "response = bot.start_interview(\n",
        "    programming_language='python',\n",
        "    experience_level='senior',\n",
        "    candidate_name='CodeLlama Test'\n",
        ")\n",
        "\n",
        "print(\"ðŸ¤– AI Interviewer:\")\n",
        "print(response)\n",
        "\n",
        "print(\"\\nðŸŽ‰ Your CodeLlama AI is ready!\")\n",
        "print(\"ðŸ’¡ Much smarter than DialoGPT for technical questions!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
