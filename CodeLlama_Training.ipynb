{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🔥 Technical Interview AI - CodeLlama Edition\n",
        "\n",
        "**Upgraded Model: CodeLlama-7B-Instruct**\n",
        "- 🧠 60x more parameters than DialoGPT\n",
        "- 💻 Built specifically for coding\n",
        "- 🎯 Perfect for technical interviews\n",
        "- ⚡ 15-20 minutes training on A100\n",
        "\n",
        "**Your GitHub Repository:** Update with your repo URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🔄 Get latest code from GitHub\n",
        "REPO_URL = 'https://github.com/shijazi88/technical-interview-ai'\n",
        "PROJECT_DIR = 'interview-ai'\n",
        "\n",
        "import os\n",
        "if os.path.exists(PROJECT_DIR):\n",
        "    print(\"🔄 Updating to latest code...\")\n",
        "    %cd $PROJECT_DIR\n",
        "    !git pull origin main\n",
        "else:\n",
        "    print(\"📥 Cloning repository...\")\n",
        "    !git clone $REPO_URL $PROJECT_DIR\n",
        "    %cd $PROJECT_DIR\n",
        "\n",
        "!ls -la *.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 📦 Install packages for CodeLlama\n",
        "!pip install -q transformers>=4.35.0 peft>=0.6.0 accelerate bitsandbytes datasets torch\n",
        "\n",
        "# Check GPU\n",
        "import torch\n",
        "print(f\"🔥 GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"💾 VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "if torch.cuda.get_device_properties(0).total_memory < 20e9:\n",
        "    print(\"⚠️ Warning: Less than 20GB VRAM - CodeLlama might not fit\")\n",
        "    print(\"💡 Consider using Mistral-7B-Instruct instead\")\n",
        "else:\n",
        "    print(\"✅ Perfect! Enough VRAM for CodeLlama-7B\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🔥 TRAIN WITH CODELLAMA (15-20 minutes)\n",
        "!python colab_training_pipeline.py \\\n",
        "    --model_name \"codellama/CodeLlama-7b-Instruct-hf\" \\\n",
        "    --num_scenarios 100 \\\n",
        "    --epochs 3 \\\n",
        "    --batch_size 2 \\\n",
        "    --learning_rate 2e-4 \\\n",
        "    --warmup_steps 100 \\\n",
        "    --max_length 2048\n",
        "\n",
        "print(\"🎉 CodeLlama training completed!\")\n",
        "print(\"🏆 You now have a professional-grade technical interview AI!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🧪 Test your upgraded AI\n",
        "from technical_interview_bot import TechnicalInterviewBot\n",
        "\n",
        "# Load your trained CodeLlama model\n",
        "bot = TechnicalInterviewBot('./technical_interview_model')\n",
        "\n",
        "# Test with a Python interview\n",
        "response = bot.start_interview(\n",
        "    programming_language='python',\n",
        "    experience_level='senior',\n",
        "    candidate_name='CodeLlama Test'\n",
        ")\n",
        "\n",
        "print(\"🤖 AI Interviewer:\")\n",
        "print(response)\n",
        "\n",
        "print(\"\\n🎉 Your CodeLlama AI is ready!\")\n",
        "print(\"💡 Much smarter than DialoGPT for technical questions!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
