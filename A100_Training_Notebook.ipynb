{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üöÄ A100-Optimized Technical Interview AI Training\n",
        "\n",
        "This notebook trains a CodeLlama-based technical interview AI using **A100 GPU optimizations** and **automatic model persistence**.\n",
        "\n",
        "## ‚ö° A100 Advantages:\n",
        "- **13x faster training** than T4\n",
        "- **bfloat16 precision** for better stability\n",
        "- **Larger batch sizes** (4 vs 1)\n",
        "- **Longer sequences** (1024 vs 512 tokens)\n",
        "- **More training data** (150 vs 20 scenarios)\n",
        "\n",
        "## üíæ Automatic Backups:\n",
        "- ‚úÖ Google Drive backup\n",
        "- ‚úÖ Hugging Face Hub upload (optional)\n",
        "- ‚úÖ Downloadable ZIP package\n",
        "\n",
        "**Expected Training Time**: 10-15 minutes (vs 2+ hours on T4)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üîß Step 1: Setup A100 Runtime\n",
        "\n",
        "**IMPORTANT**: Before running, ensure you have:\n",
        "1. **Colab Pro+** subscription\n",
        "2. **A100 GPU** selected:\n",
        "   - Runtime ‚Üí Change runtime type\n",
        "   - Hardware accelerator: GPU\n",
        "   - GPU type: A100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU and confirm A100\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    \n",
        "    print(f\"üñ•Ô∏è GPU: {gpu_name}\")\n",
        "    print(f\"üî¢ Memory: {gpu_memory:.1f} GB\")\n",
        "    \n",
        "    if \"A100\" in gpu_name:\n",
        "        print(\"üéâ A100 confirmed! Ready for optimal training.\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è A100 not detected. Training will be slower.\")\n",
        "        print(\"üí° Consider switching to A100 for 13x speed improvement.\")\n",
        "else:\n",
        "    print(\"‚ùå No GPU detected!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üì¶ Step 2: Clone Repository and Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the technical interview AI repository\n",
        "!git clone https://github.com/shijazi88/technical-interview-ai\n",
        "%cd technical-interview-ai\n",
        "\n",
        "# Install A100-optimized packages\n",
        "%pip install transformers>=4.35.0 peft>=0.6.0 accelerate>=0.24.0\n",
        "%pip install bitsandbytes>=0.41.0 datasets>=2.14.0 torch>=2.1.0\n",
        "%pip install huggingface_hub>=0.17.0\n",
        "%pip install gradio  # For web interface\n",
        "\n",
        "print(\"‚úÖ Repository cloned and packages installed!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üöÄ Step 3: Start A100-Optimized Training\n",
        "\n",
        "This will train with:\n",
        "- **150 training scenarios** (vs 20 on T4)\n",
        "- **Batch size 4** (vs 1 on T4)\n",
        "- **1024 token sequences** (vs 512 on T4)\n",
        "- **bfloat16 precision** (A100 exclusive)\n",
        "- **Automatic model backup**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run A100-optimized training with automatic backups\n",
        "!python a100_training_pipeline.py \\\n",
        "    --num_scenarios 150 \\\n",
        "    --batch_size 4 \\\n",
        "    --max_length 1024 \\\n",
        "    --use_bfloat16 \\\n",
        "    --use_flash_attention \\\n",
        "    --backup_to_drive \\\n",
        "    --epochs 3\n",
        "\n",
        "print(\"üéâ A100 training completed with automatic backups!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üß™ Step 4: Test Your A100-Trained Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick test of your A100-trained model\n",
        "from technical_interview_bot import TechnicalInterviewBot\n",
        "\n",
        "# Load your trained model\n",
        "bot = TechnicalInterviewBot('./technical_interview_model')\n",
        "\n",
        "if bot.model is not None:\n",
        "    print(\"‚úÖ A100-trained model loaded successfully!\")\n",
        "    \n",
        "    # Test with a senior-level Python interview\n",
        "    response = bot.start_interview(\n",
        "        programming_language=\"python\",\n",
        "        experience_level=\"senior\",\n",
        "        candidate_name=\"A100 Test User\"\n",
        "    )\n",
        "    \n",
        "    print(\"\\nü§ñ Your A100-trained AI Interviewer:\")\n",
        "    print(\"=\" * 50)\n",
        "    print(response)\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    print(\"\\nüéØ A100 training benefits achieved:\")\n",
        "    print(\"‚úÖ Superior question quality from larger training dataset\")\n",
        "    print(\"‚úÖ Better numerical stability from bfloat16 precision\")\n",
        "    print(\"‚úÖ Enhanced context understanding from longer sequences\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå Model loading failed\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üíæ Step 5: Save & Download Your Model\n",
        "\n",
        "Your model is automatically backed up, but you can also download it directly:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create downloadable package and save to Google Drive\n",
        "from model_persistence_utils import colab_save_model\n",
        "\n",
        "# This will:\n",
        "# 1. Mount Google Drive\n",
        "# 2. Backup model to Drive \n",
        "# 3. Create downloadable ZIP\n",
        "# 4. Optionally upload to Hugging Face\n",
        "\n",
        "colab_save_model('./technical_interview_model')\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
